{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ffvoice Python Bindings Tutorial\n",
    "\n",
    "Welcome to the ffvoice tutorial! This notebook will guide you through all the features of the ffvoice Python library.\n",
    "\n",
    "## What is ffvoice?\n",
    "\n",
    "ffvoice is a high-performance offline speech recognition library with:\n",
    "- ‚ö° **3-10x faster** than pure Python solutions\n",
    "- üîí **100% offline** - no cloud dependencies\n",
    "- üéôÔ∏è **Complete audio pipeline** - capture, denoise, VAD, transcription\n",
    "- üêç **Easy Python API** - NumPy arrays and callbacks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install ffvoice numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffvoice\n",
    "import numpy as np\n",
    "\n",
    "print(f\"ffvoice version: {ffvoice.__version__}\")\n",
    "print(f\"Available components: {[x for x in dir(ffvoice) if not x.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Whisper Speech Recognition\n",
    "\n",
    "Let's start with the core feature - speech recognition using Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Whisper\n",
    "config = ffvoice.WhisperConfig()\n",
    "config.model_type = ffvoice.WhisperModelType.TINY  # Fastest model\n",
    "config.language = \"auto\"  # Auto-detect language\n",
    "\n",
    "print(f\"Model: {ffvoice.WhisperASR.get_model_type_name(config.model_type)}\")\n",
    "print(f\"Language: {config.language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ASR (this downloads the model on first run)\n",
    "asr = ffvoice.WhisperASR(config)\n",
    "print(\"Loading model...\")\n",
    "if asr.initialize():\n",
    "    print(\"‚úì Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚úó Error: {asr.get_last_error()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe from NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample audio (1 second of silence)\n",
    "sample_rate = 48000\n",
    "audio = np.zeros(sample_rate, dtype=np.int16)\n",
    "\n",
    "print(f\"Audio shape: {audio.shape}\")\n",
    "print(f\"Audio dtype: {audio.dtype}\")\n",
    "print(f\"Duration: {len(audio)/sample_rate:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe (note: silence won't produce meaningful results)\n",
    "segments = asr.transcribe_buffer(audio)\n",
    "inference_time = asr.get_last_inference_time_ms()\n",
    "\n",
    "print(f\"Inference time: {inference_time}ms\")\n",
    "print(f\"Number of segments: {len(segments)}\")\n",
    "\n",
    "for i, seg in enumerate(segments):\n",
    "    print(f\"\\nSegment {i+1}:\")\n",
    "    print(f\"  Time: [{seg.start_ms}ms - {seg.end_ms}ms]\")\n",
    "    print(f\"  Text: {seg.text}\")\n",
    "    print(f\"  Confidence: {seg.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Noise Reduction with RNNoise\n",
    "\n",
    "RNNoise is an AI-powered noise suppression system that also provides Voice Activity Detection (VAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RNNoise\n",
    "rnnoise_config = ffvoice.RNNoiseConfig()\n",
    "rnnoise_config.enable_vad = True\n",
    "\n",
    "# Initialize\n",
    "rnnoise = ffvoice.RNNoise(rnnoise_config)\n",
    "rnnoise.initialize(sample_rate=48000, channels=1)\n",
    "print(\"‚úì RNNoise initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noisy audio (random noise)\n",
    "noisy_audio = np.random.randint(-1000, 1000, 256, dtype=np.int16)\n",
    "\n",
    "print(f\"Before processing: mean={noisy_audio.mean():.1f}, std={noisy_audio.std():.1f}\")\n",
    "\n",
    "# Process (modifies array in-place)\n",
    "rnnoise.process(noisy_audio)\n",
    "\n",
    "print(f\"After processing: mean={noisy_audio.mean():.1f}, std={noisy_audio.std():.1f}\")\n",
    "print(f\"VAD probability: {rnnoise.get_vad_probability():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Voice Activity Detection\n",
    "\n",
    "VADSegmenter intelligently segments audio based on voice activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available sensitivity presets\n",
    "print(\"VAD Sensitivity Presets:\")\n",
    "for sensitivity in [ffvoice.VADSensitivity.VERY_SENSITIVE,\n",
    "                    ffvoice.VADSensitivity.SENSITIVE,\n",
    "                    ffvoice.VADSensitivity.BALANCED,\n",
    "                    ffvoice.VADSensitivity.CONSERVATIVE,\n",
    "                    ffvoice.VADSensitivity.VERY_CONSERVATIVE]:\n",
    "    print(f\"  - {sensitivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAD with balanced sensitivity\n",
    "vad_config = ffvoice.VADConfig.from_preset(ffvoice.VADSensitivity.BALANCED)\n",
    "vad = ffvoice.VADSegmenter(vad_config, sample_rate=48000)\n",
    "\n",
    "print(\"‚úì VAD Segmenter initialized\")\n",
    "print(f\"  Threshold: {vad.get_current_threshold():.2f}\")\n",
    "print(f\"  Buffer size: {vad.get_buffer_size()} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process audio frames with callback\n",
    "segment_count = 0\n",
    "\n",
    "def on_segment(segment_array):\n",
    "    \"\"\"Called when a complete speech segment is detected\"\"\"\n",
    "    global segment_count\n",
    "    segment_count += 1\n",
    "    duration_ms = len(segment_array) / 48 # samples to ms at 48kHz\n",
    "    print(f\"Segment {segment_count}: {len(segment_array)} samples ({duration_ms:.0f}ms)\")\n",
    "\n",
    "# Simulate processing multiple frames\n",
    "for i in range(10):\n",
    "    frame = np.random.randint(-500, 500, 256, dtype=np.int16)\n",
    "    vad_prob = 0.8 if i % 3 == 0 else 0.3  # Simulate intermittent speech\n",
    "    vad.process_frame(frame, vad_prob, on_segment)\n",
    "\n",
    "# Flush remaining audio\n",
    "vad.flush(on_segment)\n",
    "\n",
    "# Print statistics\n",
    "stats = vad.get_statistics()\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Average VAD: {stats['avg_vad_prob']:.2f}\")\n",
    "print(f\"  Speech ratio: {stats['speech_ratio']:.1%}\")\n",
    "print(f\"  Is in speech: {vad.is_in_speech()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Audio File Writing\n",
    "\n",
    "Save NumPy arrays to WAV or FLAC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test audio (1-second sine wave at 440Hz)\n",
    "duration = 1.0\n",
    "frequency = 440  # Hz (A4 note)\n",
    "t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "audio = (np.sin(2 * np.pi * frequency * t) * 32767 * 0.5).astype(np.int16)\n",
    "\n",
    "print(f\"Generated {len(audio)} samples ({duration}s at {sample_rate}Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write WAV file\n",
    "wav = ffvoice.WAVWriter()\n",
    "wav.open(\"/tmp/test.wav\", sample_rate, channels=1)\n",
    "samples_written = wav.write_samples_array(audio)\n",
    "wav.close()\n",
    "\n",
    "print(f\"‚úì Wrote WAV: {samples_written} samples\")\n",
    "print(f\"  File: /tmp/test.wav\")\n",
    "print(f\"  Total samples: {wav.total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write FLAC file (compressed)\n",
    "flac = ffvoice.FLACWriter()\n",
    "flac.open(\"/tmp/test.flac\", sample_rate, channels=1, bits_per_sample=16, compression_level=5)\n",
    "samples_written = flac.write_samples_array(audio)\n",
    "compression_ratio = flac.get_compression_ratio()\n",
    "flac.close()\n",
    "\n",
    "print(f\"‚úì Wrote FLAC: {samples_written} samples\")\n",
    "print(f\"  File: /tmp/test.flac\")\n",
    "print(f\"  Compression: {compression_ratio:.2f}x\")\n",
    "print(f\"  Total samples: {flac.total_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Audio Device Information\n",
    "\n",
    "List available audio input devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PortAudio\n",
    "ffvoice.AudioCapture.initialize()\n",
    "\n",
    "# Get devices\n",
    "devices = ffvoice.AudioCapture.get_devices()\n",
    "\n",
    "print(f\"Found {len(devices)} audio device(s):\\n\")\n",
    "\n",
    "for dev in devices:\n",
    "    default_marker = \" [DEFAULT]\" if dev.is_default else \"\"\n",
    "    print(f\"Device {dev.id}: {dev.name}{default_marker}\")\n",
    "    print(f\"  Input channels: {dev.max_input_channels}\")\n",
    "    print(f\"  Output channels: {dev.max_output_channels}\")\n",
    "    print(f\"  Supported sample rates: {dev.supported_sample_rates[:5]}...\")\n",
    "    print()\n",
    "\n",
    "# Get default device\n",
    "default_dev = ffvoice.AudioCapture.get_default_input_device()\n",
    "if default_dev:\n",
    "    print(f\"Default input device: {default_dev.name}\")\n",
    "\n",
    "# Cleanup\n",
    "ffvoice.AudioCapture.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Real-time Pipeline\n",
    "\n",
    "This example demonstrates the full pipeline:\n",
    "AudioCapture ‚Üí RNNoise ‚Üí VADSegmenter ‚Üí WhisperASR\n",
    "\n",
    "**Note:** This requires a microphone and should be run in a local environment, not in a cloud notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration - won't work without a microphone\n",
    "# See complete_realtime_pipeline.py for a working example\n",
    "\n",
    "class RealtimePipeline:\n",
    "    def __init__(self):\n",
    "        # Initialize all components\n",
    "        self.rnnoise = ffvoice.RNNoise(ffvoice.RNNoiseConfig())\n",
    "        self.rnnoise.initialize(48000, 1)\n",
    "        \n",
    "        vad_config = ffvoice.VADConfig.from_preset(ffvoice.VADSensitivity.BALANCED)\n",
    "        self.vad = ffvoice.VADSegmenter(vad_config, 48000)\n",
    "        \n",
    "        config = ffvoice.WhisperConfig()\n",
    "        config.model_type = ffvoice.WhisperModelType.TINY\n",
    "        self.asr = ffvoice.WhisperASR(config)\n",
    "        self.asr.initialize()\n",
    "        \n",
    "        ffvoice.AudioCapture.initialize()\n",
    "        self.capture = ffvoice.AudioCapture()\n",
    "        self.capture.open(48000, 1, 256)\n",
    "    \n",
    "    def on_segment(self, segment_array):\n",
    "        \"\"\"Transcribe complete speech segments\"\"\"\n",
    "        segments = self.asr.transcribe_buffer(segment_array)\n",
    "        for seg in segments:\n",
    "            print(f\"‚Üí {seg.text}\")\n",
    "    \n",
    "    def on_audio(self, audio_array):\n",
    "        \"\"\"Process each audio frame\"\"\"\n",
    "        # 1. Denoise\n",
    "        self.rnnoise.process(audio_array)\n",
    "        \n",
    "        # 2. Get VAD and segment\n",
    "        vad_prob = self.rnnoise.get_vad_probability()\n",
    "        self.vad.process_frame(audio_array, vad_prob, self.on_segment)\n",
    "    \n",
    "    def run(self, duration=5):\n",
    "        \"\"\"Run pipeline for specified duration\"\"\"\n",
    "        import time\n",
    "        print(f\"Recording for {duration} seconds...\")\n",
    "        self.capture.start(self.on_audio)\n",
    "        time.sleep(duration)\n",
    "        self.capture.stop()\n",
    "        self.vad.flush(self.on_segment)\n",
    "        \n",
    "        # Cleanup\n",
    "        self.capture.close()\n",
    "        ffvoice.AudioCapture.terminate()\n",
    "\n",
    "# Uncomment to run (requires microphone)\n",
    "# pipeline = RealtimePipeline()\n",
    "# pipeline.run(duration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "1. ‚úÖ Transcribe audio with Whisper ASR\n",
    "2. ‚úÖ Process NumPy arrays\n",
    "3. ‚úÖ Reduce noise with RNNoise\n",
    "4. ‚úÖ Detect voice activity with VAD\n",
    "5. ‚úÖ Write audio files (WAV/FLAC)\n",
    "6. ‚úÖ List audio devices\n",
    "7. ‚úÖ Build complete pipelines\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the [complete_realtime_pipeline.py](../../examples/complete_realtime_pipeline.py) example\n",
    "- Read the [Quick Start Guide](../QUICKSTART.md)\n",
    "- Check the [API Reference](../../README.md#api-reference)\n",
    "- Explore [performance benchmarks](../../README.md#performance)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- GitHub: https://github.com/chicogong/ffvoice-engine\n",
    "- Issues: https://github.com/chicogong/ffvoice-engine/issues\n",
    "- Documentation: https://github.com/chicogong/ffvoice-engine/tree/master/docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
